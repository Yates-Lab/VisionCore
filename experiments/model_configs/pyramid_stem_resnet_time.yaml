# V1 multidataset model configuration with ResNet convnet using PyramidStem
# This model uses a Laplacian Pyramid stem that decomposes the input into multiple scales
# before processing through the ResNet backbone
model_type: v1multi

# Model dimensions
sampling_rate: 240
initial_input_channels: 1 # number of channels in the input data

# Frontend configuration
adapter:
  type: adapter
  params: {grid_size: 51, init_sigma: 1, transform: scale}

frontend:
  type: learnable_temporal
  params: {kernel_size: 16, num_channels: 4, init_type: gaussian_derivatives,
          aa_signal: true,
          causal: true, bias: false}

# Convnet configuration - ResNet with PyramidStem
convnet:
  type: resnet
  params:
    final_activation: softplus
    channels: [32, 256, 128]
    dim: 3
    checkpointing: false
    
    # PyramidStem configuration
    stem_config:
      type: pyramid  # Use PyramidStem instead of default ConvBlock
      Nlevels: 3  # Number of pyramid levels
      out_channels: 16  # Output channels per pyramid level
      kernel_size: 15  # Kernel size for convolution at each level
      stride: 1
      padding: same
      bias: false
      amplitude_cat: false  # Concatenate amplitude information
      aa_signal: true  # Apply signal-domain anti-aliasing
      aa_freq: true  # Apply frequency-domain anti-aliasing
    
    block_configs:
      # Stage 0: keep full res, minimal pooling
      - conv_params:
          type: standard
          kernel_size: [3, 3, 3]
          padding:    [1, 1, 1]
          stride:     1
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params: null

      # Stage 1: first downsample
      - conv_params:
          type: standard
          kernel_size: [3, 3, 3]
          padding:    [1, 1, 1]
          stride:     1
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params:
          type: aablur
          stride: 2
          r: 8
          use_soft: true

      # Stage 2: second downsample with dilation
      - conv_params:
          type: standard
          kernel_size: [3, 3, 3]
          padding:    [1, 1, 1]
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params:
          type: aablur
          stride: 1
          r: 16
          use_soft: true

# Modulator
modulator:
  type: convgru
  params:
    behavior_dim: 42
    feature_dim: 256 
    hidden_dim: 256  
    beh_emb_dim: 32  
    kernel_size: 3   
    
    # ConvGRU Optimizations
    use_layer_norm: true      
    learnable_h0: true         
    use_depthwise: false       
    use_grouped: false         
    num_groups: 1              
    use_residual: false        
    grad_clip_val: null      
    
# Recurrent configuration
recurrent:
  type: none
  params: {}

# Readout configuration
readout:
  type: gaussian
  params:
    n_units: 8
    bias: true
    initial_std: 5.0
    initial_mean_scale: 0.1

output_activation: none

regularization:
  - name: readout_sparsity
    type: l1
    lambda: 1.0e-7
    apply_to: ["readouts/features"]
    schedule:
      kind: warmup
      start_epoch: 1
  - name: decay_readout_std
    type: proximal_clamp_positive
    lambda: 1.5  # final max std value
    apply_to: ["readouts/std"]
    schedule:
      kind: linear_decay
      start_epoch: 0
      end_epoch: 50
      start_lambda: 10.0  # initial max 
  - name: exclude_means_from_wd
    type: l2
    lambda: 0.0  # No penalty, but excludes from weight decay
    apply_to: ["readouts/mean"]
    schedule:
      kind: constant

