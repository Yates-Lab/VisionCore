# V1 multidataset model configuration with DenseNet convnet and ConvGRU modulation
# This model has shared components (frontend, DenseNet, modulator)
# and experiment specific components (adapter, readout)
# The ConvGRU behavior modulator processes features and behavior through a ConvGRU
model_type: v1multi

# Model dimensions
sampling_rate: 240
initial_input_channels: 1 # number of channels in the input data

# Frontend configuration
adapter:
  type: adapter
  params: {grid_size: 51, init_sigma: 1, transform: scale}

frontend:
  type: learnable_temporal
  params: {kernel_size: 16, num_channels: 4, init_type: gaussian_derivatives,
          causal: true, bias: false}

# Convnet configuration - DenseNet with similar capacity to ResNet
convnet:
  type: densenet
  params:
    final_activation: softplus
    channels: [8, 128, 96]  # Growth channels that will accumulate: 4 -> 12 -> 76 -> 140 final
    dim: 3
    checkpointing: false
    block_config:
      conv_params:
        type: depthwise
        kernel_size: [3, 5, 5]
        padding:    [1, 3, 3]
        stride:     [1, 2, 2]
      norm_type: rms
      act_type: silu
      dropout: 0.1

# Modulator configuration - ConvGRU
modulator:
  type: convgru
  params:
    behavior_dim: 42
    feature_dim: 140  # Number of convnet output channels (matches final DenseNet stage)
    hidden_dim: 128   # Slightly reduced from 140 for stability
    beh_emb_dim: 32   # Behavior embedding dimension (increased for better capacity)
    kernel_size: 3    # ConvGRU kernel size

# Recurrent configuration
recurrent:
  type: none
  params: {}

# Readout configuration
readout:
  type: gaussian
  params:
    n_units: 8
    bias: true
    initial_std: 5.0
    initial_mean_scale: 0.1

output_activation: none

regularization:
  - name: readout_sparsity
    type: l1
    lambda: 1.0e-7
    apply_to: ["readouts/features"]
    schedule:
      kind: warmup
      start_epoch: 1
  - name: decay_readout_std
    type: proximal_clamp_positive
    lambda: 1.5  # final max std value
    apply_to: ["readouts/std"]  # matches readouts.X.std_*
    schedule:
      kind: linear_decay
      start_epoch: 0
      end_epoch: 50
      start_lambda: 10.0  # initial max 
  - name: exclude_means_from_wd
    type: l2
    lambda: 0.0  # No penalty, but excludes from weight decay
    apply_to: ["readouts/mean"]
    schedule:
      kind: constant

# ConvGRU Modulator specific notes:
# - ConvGRU modulator outputs 128 channels (hidden_dim)
# - hidden_dim=128 slightly less than feature_dim=140 for stability
# - beh_emb_dim=32 provides good behavior representation capacity
# - No auxiliary loss needed (unlike PC modulator)
# - Processes all timesteps through ConvGRU and outputs final hidden state

# DenseNet specific notes:
# - DenseNet channels accumulate: input_channels + sum(previous_channels)
# - With channels [8, 64, 64]: 4 -> 12 -> 76 -> 140 final channels
# - This provides similar capacity to ResNet [8, 256, 128] = 128 final
# - DenseNet is more parameter efficient due to feature reuse
