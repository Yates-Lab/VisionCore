# V1 multidataset model configuration with ResNet convnet and ConvGRU recurrent stage
# This model has shared components (frontend, ResNet, recurrent ConvGRU)
# and experiment specific components (adapter, readout)
# The ConvGRU is used as a recurrent stage (not modulator) with no behavioral input
model_type: v1multi

# Model dimensions
sampling_rate: 240
initial_input_channels: 1 # number of channels in the input data

# Frontend configuration
adapter:
  type: adapter
  params: {grid_size: 51, init_sigma: 1, transform: scale}

frontend:
  type: learnable_temporal
  params: {kernel_size: 16, num_channels: 4, init_type: gaussian_derivatives,
          causal: true, bias: false}

# Convnet configuration - ResNet with per-layer block configs (same as learned_res_small_gru)
convnet:
  type: resnet
  params:
    final_activation: softplus
    channels: [8, 256, 128]
    dim: 3
    checkpointing: false
    block_configs:

      # Stage 0: keep full res, no pooling
      - conv_params:
          type: depthwise
          kernel_size: [1, 7, 7]
          padding:    [0, 0, 0]
          stride:     1
          # padding_mode: reflect
        norm_type: rms
        act_type: silu
        dropout: 0.1              # <-- leave off
        pool_params: null

      # Stage 1: 
      - conv_params:
          type: depthwise
          kernel_size: [1, 7, 7]
          padding:    [0,0,0]
          stride:     1
        norm_type: rms
        act_type: silu
        dropout: 0.1
        pool_params: null

      # Stage 2: second (final) downsample, then dilation
      - conv_params:
          type: depthwise
          kernel_size: [1, 7, 7]
          padding:    [0,0,0] 
          dilation:   1           # enlarge RF, keep 13Ã—13 map
          stride: 1
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params:
          type: aablur
          stride: 2
          r: 8
          use_soft: true
          
# Modulator configuration - ConvGRU
modulator:
  type: convgru
  params:
    behavior_dim: 42
    feature_dim: 128  # Number of convnet output channels (matches final ResNet stage)
    hidden_dim: 128   # Match feature_dim for stability
    beh_emb_dim: 32   # Behavior embedding dimension (increased for better capacity)
    kernel_size: 3    # ConvGRU kernel size
    fast_phase: true


# Readout configuration
readout:
  type: gaussian
  params:
    n_units: 8
    bias: true
    initial_std: .5
    initial_mean_scale: 0.1

output_activation: none

regularization:
  - name: readout_sparsity
    type: l1
    lambda: 1.0e-7
    apply_to: ["readouts/features"]
    schedule:
      kind: warmup
      start_epoch: 1
  - name: decay_readout_std
    type: proximal_clamp_positive
    lambda: 1.5  # final max std value
    apply_to: ["readouts/std"]  # matches readouts.X.std_*
    schedule:
      kind: linear_decay
      start_epoch: 0
      end_epoch: 50
      start_lambda: 10.0  # initial max 
  - name: exclude_means_from_wd
    type: l2
    lambda: 0.0  # No penalty, but excludes from weight decay
    apply_to: ["readouts/mean"]
    schedule:
      kind: constant

# ConvGRU Recurrent specific notes:
# - ConvGRU recurrent outputs 128 channels (hidden_dim)
# - hidden_dim=128 matches convnet output channels for consistency
# - No behavior input - pure temporal processing
# - Processes all timesteps through ConvGRU and outputs final hidden state
# - Same ConvGRU implementation as modulator but without behavior embedding
