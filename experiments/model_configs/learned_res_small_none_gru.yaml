# V1 multidataset model configuration with ResNet convnet and ConvGRU recurrent stage
# This model has shared components (frontend, ResNet, recurrent ConvGRU)
# and experiment specific components (adapter, readout)
# The ConvGRU is used as a recurrent stage (not modulator) with no behavioral input
model_type: v1multi

# Model dimensions
sampling_rate: 240
initial_input_channels: 1 # number of channels in the input data

# Frontend configuration
adapter:
  type: adapter
  params: {grid_size: 51, init_sigma: 1, transform: scale}

frontend:
  type: learnable_temporal
  params: {kernel_size: 16, num_channels: 4, init_type: gaussian_derivatives,
          aa_signal: true,
          causal: true, bias: false}

# Convnet configuration - ResNet with per-layer block configs (same as learned_res_small_gru)
convnet:
  type: resnet
  params:
    final_activation: softplus
    channels: [8, 256, 128]
    dim: 3
    checkpointing: false
    block_configs:

      # Stage 0: keep full res, no pooling
      - conv_params:
          type: standard
          kernel_size: [3, 11, 11]
          padding:    [1, 3, 3]
          stride:     2
          aa_signal: true
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params: null

      # Stage 1: 
      - conv_params:
          type: standard
          kernel_size: [3, 5, 5]
          padding:    [1, 3, 3]
          stride:     1
        norm_type: rms
        act_type: silu
        dropout: 0.1
        pool_params:
          type: aablur
          stride: 2
          r: 8
          use_soft: true

      # Stage 2: second (final) downsample, then dilation
      - conv_params:
          type: standard
          kernel_size: [3, 3, 3]
          padding:    [2,2,2] 
          dilation:   2           # enlarge RF, keep 13Ã—13 map
          stride: 1
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params:
          type: aablur
          stride: 1
          r: 16
          use_soft: true
          

# Modulator configuration - None (no behavioral modulation)
modulator:
  type: none
  params: {}

# Recurrent configuration - ConvGRU
recurrent:
  type: convgru
  params:
    hidden_dim: 128   # Match convnet output channels for stability
    kernel_size: 3    # ConvGRU kernel size

# Readout configuration
readout:
  type: gaussian
  params:
    n_units: 8
    bias: true
    initial_std: 5.0
    initial_mean_scale: 0.1

output_activation: none

regularization:
  - name: readout_sparsity
    type: l1
    lambda: 1.0e-7
    apply_to: ["readouts/features"]
    schedule:
      kind: warmup
      start_epoch: 1
  - name: decay_readout_std
    type: proximal_clamp_positive
    lambda: 1.5  # final max std value
    apply_to: ["readouts/std"]  # matches readouts.X.std_*
    schedule:
      kind: linear_decay
      start_epoch: 0
      end_epoch: 50
      start_lambda: 10.0  # initial max 
  - name: exclude_means_from_wd
    type: l2
    lambda: 0.0  # No penalty, but excludes from weight decay
    apply_to: ["readouts/mean"]
    schedule:
      kind: constant
