# V1 model configuration with DenseNet convnet, concat modulator, ConvGRU recurrent stage, and spike history processing
model_type: v1multi_history

# Model dimensions
initial_input_channels: 1

# Adapter configuration
adapter:
  type: adapter
  params:
    init_sigma: 1.0
    grid_size: 51
    transform: scale

# Frontend configuration
frontend:
  type: learnable_temporal
  params:
    kernel_size: 16
    num_channels: 4
    init_type: biphasic
    split_MP: False
    aa_signal: true
    aa_window: hann
    aa_window_kwargs:
      power: .25
    use_weight_norm: false
    keep_unit_norm: false
    causal: true
    bias: true

# Convnet configuration - DenseNet
convnet:
  type: densenet
  params:
    channels: [16, 32]
    checkpointing: false
    stem_config:
      out_channels: 8
      conv_params:
        type: standard
        kernel_size: [1, 7, 7]
        padding: [0, 0, 0]
        stride: 1
        dilation: [1, 1, 1]
        aa_signal: true
        weight_norm_dim: 0
        use_weight_norm: true
        keep_unit_norm: true
      norm_type: grn
      act_type: splitrelu
      pool_params: null
      dropout: 0.0
      order: [pad, conv, norm, act]
    block_configs:
      - conv_params:
          type: standard
          kernel_size: [3, 9, 9]
          aa_signal: true
          aa_window: hann
          aa_window_kwargs:
            power: 1.0
          use_weight_norm: true
          keep_unit_norm: true
          padding: [0, 0, 0]
        norm_type: grn
        act_type: splitrelu
        dropout: 0.1
        pool_params:
          type: max
          kernel_size: 2
          stride: 2
      - conv_params:
          type: standard
          kernel_size: [3, 5, 5]
          use_weight_norm: true
          keep_unit_norm: true
          aa_signal: true
          padding: [0, 0, 0]
        norm_type: grn
        act_type: splitrelu
        dropout: 0.1
        pool_params: null

# Modulator configuration - Concat
modulator:
  type: concat
  params:
    behavior_dim: 42
    feature_dim: 32

# Recurrent configuration - ConvGRU
recurrent:
  type: convgru
  params:
    n_layers: 1
    hidden_dim: 128
    kernel_size: 3

# Readout configuration
readout:
  type: gaussian
  params:
    n_units: 8
    bias: true
    initial_std: 5.0
    initial_mean_scale: 0.1

# Spike history encoder configuration
history_encoder:
  type: mlp
  params:
    num_lags: 5  # Number of spike history lags to use
    hidden_dims: [32, 10]  # Bottleneck architecture
    act_type: relu
    spectral_norm: true  # Use spectral normalization for stability during simulation
    bias: true
    dropout: 0.0
    output_activation: false

output_activation: softplus

regularization:
  - name: readout_sparsity
    type: l1
    lambda: 1.0e-7
    apply_to: ["readouts/features"]
    schedule:
      kind: warmup
      start_epoch: 90
  # - name: decay_readout_std
  #   type: proximal_clamp_positive
  #   lambda: 1.5  # final max std value
  #   apply_to: ["readouts/std"]  # matches readouts.X.std_*
  #   schedule:
  #     kind: linear_decay
  #     start_epoch: 0
  #     end_epoch: 50
  #     start_lambda: 10.0  # initial max 
  # - name: exclude_means_from_wd
  #   type: l2
  #   lambda: 0.0  # No penalty, but excludes from weight decay
  #   apply_to: ["readouts/mean"]
  #   schedule:
  #     kind: constant

