# V1 multidataset model configuration with ResNet convnet, ConvGRU modulation, and ZIP loss
# This model uses Zero-Inflated Poisson loss for handling excess zeros in neural data
# Based on res_small_gru.yaml with ZIP loss enabled
model_type: v1multi

# Model dimensions
sampling_rate: 240
initial_input_channels: 1 # number of channels in the input data

# Frontend configuration
adapter:
  type: adapter
  params: {grid_size: 51, init_sigma: 1.0, transform: scale}

frontend:
  type: none
  params: {}

# Convnet configuration - ResNet with per-layer block configs
convnet:
  type: resnet
  params:
    final_activation: softplus
    channels: [8, 256, 128]
    dim: 3
    checkpointing: false
    block_configs:

      # Stage 0: keep full res, no pooling
      - conv_params:
          type: standard
          kernel_size: [3, 5, 5]
          padding:    [1, 3, 3]
          stride:     2
        norm_type: rms
        act_type: silu
        dropout: 0.0              # <-- leave off
        pool_params: null

      # Stage 1: 
      - conv_params:
          type: standard
          kernel_size: [3, 5, 5]
          padding:    [1, 3, 3]
          stride:     1
        norm_type: rms
        act_type: silu
        dropout: 0.1
        pool_params:
          type: aablur
          stride: 2
          r: 8
          use_soft: true

      # Stage 2: second (final) downsample, then dilation
      - conv_params:
          type: standard
          kernel_size: [3, 3, 3]
          padding:    [2,2,2] 
          dilation:   2           # enlarge RF, keep 13Ã—13 map
          stride: 1
        norm_type: rms
        act_type: silu
        dropout: 0.2
        pool_params:
          type: aablur
          stride: 1
          r: 16
          use_soft: true
          

# Modulator configuration - ConvGRU
modulator:
  type: convgru
  params:
    behavior_dim: 42
    feature_dim: 128  # Number of convnet output channels (matches final ResNet stage)
    hidden_dim: 128   # Match feature_dim for stability
    beh_emb_dim: 32   # Behavior embedding dimension (increased for better capacity)
    kernel_size: 3    # ConvGRU kernel size

# Recurrent configuration
recurrent:
  type: none
  params: {}

# Readout configuration
# NOTE: Standard readout only outputs lambda (Poisson rate)
# For full ZIP functionality, you would need a custom readout that outputs both lambda and pi
readout:
  type: gaussian
  params:
    n_units: 8
    bias: true
    initial_std: 5.0
    initial_mean_scale: 0.1

# Use identity activation for log-space predictions (recommended for ZIP)
output_activation: none

# ============================================================================
# ZERO-INFLATED POISSON LOSS CONFIGURATION
# ============================================================================
# Set loss_type to 'zip' or 'zero_inflated_poisson' to use ZIP loss
# Note: Current implementation falls back to standard Poisson if model doesn't output 'pi'
loss_type: zip

regularization:
  - name: readout_sparsity
    type: l1
    lambda: 1.0e-7
    apply_to: ["readouts/features"]
    schedule:
      kind: warmup
      start_epoch: 1
  - name: decay_readout_std
    type: proximal_clamp_positive
    lambda: 1.5  # final max std value
    apply_to: ["readouts/std"]  # matches readouts.X.std_*
    schedule:
      kind: linear_decay
      start_epoch: 0
      end_epoch: 50
      start_lambda: 10.0  # initial max 
  - name: exclude_means_from_wd
    type: l2
    lambda: 0.0  # No penalty, but excludes from weight decay
    apply_to: ["readouts/mean"]
    schedule:
      kind: constant

# ZIP Loss Notes:
# ---------------
# 1. The current readout only outputs lambda (Poisson rate parameter)
# 2. Without a 'pi' output, the loss falls back to standard Poisson NLL
# 3. For full ZIP functionality, you need to:
#    - Create a custom readout that outputs both lambda and pi
#    - Modify the forward pass to return both parameters
#    - Ensure the batch dictionary includes 'pi' key
# 4. See docs/zero_inflated_poisson_loss.md for more details

